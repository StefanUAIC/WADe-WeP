<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>WeP - Web News Provenance: Technical Report</title>
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        window.addEventListener('load', function() {
            mermaid.initialize({ 
                startOnLoad: true, 
                theme: 'default',
                securityLevel: 'loose'
            });
        });
    </script>
    <style>
        body {
            font-family: 'Georgia', serif;
            line-height: 1.6;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
            color: #333;
        }
        h1 { font-size: 2.5em; margin-top: 0; border-bottom: 3px solid #333; padding-bottom: 10px; }
        h2 { font-size: 1.8em; margin-top: 40px; border-bottom: 2px solid #666; padding-bottom: 5px; }
        h3 { font-size: 1.4em; margin-top: 30px; color: #444; }
        code { background: #f4f4f4; padding: 2px 6px; border-radius: 3px; font-family: 'Courier New', monospace; }
        pre { background: #f4f4f4; padding: 15px; border-radius: 5px; overflow-x: auto; }
        pre code { background: none; padding: 0; }
        a { color: #0066cc; text-decoration: none; }
        a:hover { text-decoration: underline; }
        .authors { font-style: italic; color: #666; margin: 20px 0; }
        .mermaid { background: white; padding: 20px; border-radius: 5px; margin: 20px 0; }
        figure { margin: 30px 0; }
        figcaption { text-align: center; font-style: italic; color: #666; margin-top: 10px; }
    </style>
</head>
<body prefix="schema: http://schema.org" vocab="http://schema.org/" typeof="ScholarlyArticle">
    <header>
        <h1 property="headline">WeP - Web News Provenance: Technical Report</h1>
        <div class="authors">
            <span property="author" typeof="Person">
                <span property="name">Stefan Catiru</span>
            </span> &amp;
            <span property="author" typeof="Person">
                <span property="name">Matei Maxim</span>
            </span>
        </div>
        <p property="datePublished">January 2026</p>
    </header>

    <section id="abstract" role="doc-abstract">
        <h2>Abstract</h2>
        <p property="abstract">
            WeP (Web News Provenance) is a semantic web application designed to track, model, and manage
            the complete provenance chain of online newspaper articles. Built using W3C PROV standards,
            the system provides comprehensive metadata management using Dublin Core (DCMI) and IPTC standards,
            entity linking with DBpedia and Wikidata, and machine learning-based recommendation systems.
            The application exposes a SPARQL endpoint with RDFa and JSON-LD support, enabling semantic
            queries over news article provenance data. Results demonstrate successful integration of
            semantic web technologies for news provenance tracking with SHACL validation and interactive
            visualization capabilities.
        </p>
    </section>

    <section id="introduction" role="doc-introduction">
        <h2>Introduction</h2>
        <p>
            In the era of digital journalism and online news dissemination, tracking the provenance
            of news articles has become increasingly important. The ability to trace the origin,
            transformations, and derivations of news content is crucial for combating misinformation,
            ensuring journalistic integrity, and understanding information flow in the digital age.
        </p>
        <p>
            WeP (Web News Provenance) addresses this challenge by providing a comprehensive platform
            for modeling and managing news article provenance using semantic web technologies. The system
            implements W3C PROV ontology to capture the complete lifecycle of news articles, including
            their creation, modification, translation, and derivation from other sources.
        </p>
        <p>
            The platform enables users to perform key operations:
        </p>
        <ul>
            <li><strong>Create and manage</strong> news articles with rich metadata (DCMI, IPTC standards)</li>
            <li><strong>Query</strong> provenance data using SPARQL with advanced filtering capabilities</li>
            <li><strong>Visualize</strong> provenance chains interactively using D3.js knowledge graphs</li>
            <li><strong>Recommend</strong> related articles using machine learning (TF-IDF + Cosine Similarity)</li>
            <li><strong>Validate</strong> RDF data using SHACL constraints</li>
            <li><strong>Link</strong> to external knowledge bases (DBpedia, Wikidata)</li>
        </ul>
        <p>
            The backend is implemented in Python using FastAPI, serving data from Apache Jena Fuseki
            SPARQL endpoint. The frontend is built with React and Tailwind CSS, providing an accessible
            and responsive user interface compliant with WCAG 2.1 AA standards.
        </p>
        <p>
            This technical report provides detailed documentation of the system architecture, data models,
            API implementation, and integration with external semantic web resources.
        </p>
    </section>

    <section id="architecture">
        <h2>System Architecture</h2>
        <p>
            WeP follows a microservices-based architecture with three main components:
            Frontend (React), Backend (Python FastAPI), and Triplestore (Apache Jena Fuseki).
        </p>

        <h3>System Context Diagram (C4 Model - Level 1)</h3>
        <figure>
            <div class="mermaid">
graph TB
    User[User/Journalist]
    WeP[WeP System<br/>Web News Provenance]
    DBpedia[DBpedia<br/>External Knowledge Base]
    Wikidata[Wikidata<br/>External Knowledge Base]

    User -->|Creates articles,<br/>queries provenance| WeP
    WeP -->|Entity extraction,<br/>SPARQL queries| DBpedia
    WeP -->|Entity linking,<br/>semantic enrichment| Wikidata

    style WeP fill:#4CAF50,stroke:#333,stroke-width:3px,color:#fff
    style User fill:#2196F3,stroke:#333,stroke-width:2px,color:#fff
    style DBpedia fill:#FF9800,stroke:#333,stroke-width:2px,color:#fff
    style Wikidata fill:#9C27B0,stroke:#333,stroke-width:2px,color:#fff
            </div>
            <figcaption>Figure 1: System Context - WeP interacts with users and external knowledge bases</figcaption>
        </figure>

        <h3>Container Diagram (C4 Model - Level 2)</h3>
        <figure>
            <div class="mermaid">
graph TB
    User[User Browser]

    subgraph WeP[WeP System]
        Frontend[React Frontend<br/>Port 3000<br/>Tailwind CSS, D3.js]
        Backend[Python Backend<br/>Port 8000<br/>FastAPI, RDFLib]
        Fuseki[Apache Jena Fuseki<br/>Port 3030<br/>SPARQL Endpoint]
    end

    DBpedia[DBpedia Spotlight API]
    Wikidata[Wikidata SPARQL]

    User -->|HTTP/HTTPS| Frontend
    Frontend -->|REST API| Backend
    Backend -->|SPARQL queries| Fuseki
    Backend -->|Entity extraction| DBpedia
    Backend -->|Entity linking| Wikidata

    style Frontend fill:#61DAFB,stroke:#333,stroke-width:2px
    style Backend fill:#3776AB,stroke:#333,stroke-width:2px,color:#fff
    style Fuseki fill:#E76F00,stroke:#333,stroke-width:2px,color:#fff
    style User fill:#2196F3,stroke:#333,stroke-width:2px,color:#fff
            </div>
            <figcaption>Figure 2: Container Diagram - Main system components and their interactions</figcaption>
        </figure>

        <h3>Component Diagram (C4 Model - Level 3)</h3>
        <figure>
            <div class="mermaid">
graph TB
    subgraph Frontend[React Frontend]
        ArticleList[Article List]
        ArticleDetail[Article Detail]
        ProvenanceGraph[Provenance Graph<br/>D3.js]
        SPARQLQuery[SPARQL Query]
        EntityExplorer[Entity Explorer]
    end

    subgraph Backend[Python Backend]
        ArticleAPI[Article API]
        FusekiService[Fuseki Service]
        DBpediaService[DBpedia Service]
        RecommendationService[ML Service]
        SHACLService[SHACL Service]
    end

    ArticleList --> ArticleAPI
    ArticleDetail --> ArticleAPI
    ProvenanceGraph --> ArticleAPI
    SPARQLQuery --> FusekiService
    EntityExplorer --> DBpediaService

    ArticleAPI --> FusekiService
    ArticleAPI --> DBpediaService
    ArticleAPI --> RecommendationService
    ArticleAPI --> SHACLService

    style Frontend fill:#E3F2FD,stroke:#1976D2,stroke-width:2px
    style Backend fill:#FFF3E0,stroke:#F57C00,stroke-width:2px
            </div>
            <figcaption>Figure 3: Component Diagram - Internal components</figcaption>
        </figure>

        <h3>Code Diagram (C4 Model - Level 4)</h3>
        <p>
            Detailed view of the Recommendation Service implementation showing the ML pipeline:
        </p>
        <figure>
            <div class="mermaid">
classDiagram
    class RecommendationService {
        +get_ml_recommendations(current, all_articles, limit)
        -vectorize_text(articles)
        -calculate_similarity(vectors)
        -rank_results(similarities)
    }

    class TfidfVectorizer {
        +fit_transform(texts)
        +transform(text)
        -build_vocabulary()
        -calculate_idf()
    }

    class CosineSimilarity {
        +compute(vector1, vector2)
        -dot_product()
        -magnitude()
    }

    class Article {
        +String id
        +String title
        +String content
        +List keywords
    }

    RecommendationService --> TfidfVectorizer : uses
    RecommendationService --> CosineSimilarity : uses
    RecommendationService --> Article : processes
    TfidfVectorizer --> Article : extracts text
    CosineSimilarity --> TfidfVectorizer : compares vectors
            </div>
            <figcaption>Figure 4: Code-level diagram of ML Recommendation Service</figcaption>
        </figure>

        <h3>Deployment Architecture</h3>
        <figure>
            <div class="mermaid">
graph TB
    subgraph AWS[AWS Cloud - eu-west-1]
        subgraph EC2[EC2 Instance - t3.medium]
            Docker[Docker Engine]
            subgraph Containers[Docker Containers]
                Frontend[Frontend Container<br/>React + Nginx<br/>Port 3000]
                Backend[Backend Container<br/>Python FastAPI<br/>Port 8000]
                Fuseki[Fuseki Container<br/>Apache Jena<br/>Port 3030]
            end
        end
        EBS[EBS Volume<br/>Persistent Storage]
    end

    Internet[Internet Users]

    Internet -->|HTTP| EC2
    Docker --> Containers
    Fuseki --> EBS

    style AWS fill:#FF9900,stroke:#232F3E,stroke-width:3px,color:#fff
    style EC2 fill:#EC7211,stroke:#232F3E,stroke-width:2px
    style Containers fill:#146EB4,stroke:#232F3E,stroke-width:2px
            </div>
            <figcaption>Figure 5: Deployment architecture on AWS EC2</figcaption>
        </figure>
    </section>

    <section id="data-structures">
        <h2>Internal Data Structures</h2>

        <h3>RDF Data Model</h3>
        <p>
            WeP stores all data in RDF (Resource Description Framework) format using the triple structure:
            subject-predicate-object. The application uses multiple vocabularies to ensure semantic
            interoperability:
        </p>

        <ul>
            <li><strong>W3C PROV</strong> - Provenance ontology (Entity, Activity, Agent)</li>
            <li><strong>Dublin Core (DCMI)</strong> - Basic metadata (dc:title, dc:creator, dcterms:created)</li>
            <li><strong>Schema.org</strong> - NewsArticle, Person, Organization</li>
            <li><strong>IPTC</strong> - News-specific subject codes</li>
        </ul>

        <h3>Article Data Model</h3>
        <figure>
            <div class="mermaid">
classDiagram
    class NewsArticle {
        +URI id
        +String title
        +String content
        +String author
        +String publication
        +String language
        +DateTime created
        +List~String~ keywords
        +List~String~ iptc_subjects
        +List~URI~ image_urls
        +List~URI~ video_urls
        +List~URI~ audio_urls
        +URI based_on
        +String derivation_type
    }

    class Activity {
        +URI id
        +DateTime startTime
        +DateTime endTime
    }

    class Agent {
        +URI id
        +String name
    }

    NewsArticle --|> Entity : is a
    NewsArticle --> Activity : wasGeneratedBy
    Activity --> Agent : wasAssociatedWith
    NewsArticle --> NewsArticle : wasDerivedFrom
    NewsArticle --> DBpediaEntity : mentions
    NewsArticle --> WikidataEntity : sameAs
            </div>
            <figcaption>Figure 3: Article data model with W3C PROV relationships</figcaption>
        </figure>

        <h3>RDF Triple Examples</h3>
        <figure typeof="schema:SoftwareSourceCode">
            <pre><code>
@prefix prov: &lt;http://www.w3.org/ns/prov#&gt; .
@prefix dc: &lt;http://purl.org/dc/elements/1.1/&gt; .
@prefix schema: &lt;http://schema.org/&gt; .
@prefix iptc: &lt;http://iptc.org/std/Iptc4xmpExt/2008-02-29/&gt; .

&lt;article/123&gt; a schema:NewsArticle, prov:Entity ;
    dc:title "NASA Artemis Mission" ;
    dc:creator "Dr. Robert Williams" ;
    schema:headline "NASA Artemis Mission" ;
    schema:author "Dr. Robert Williams" ;
    schema:articleBody "NASA successfully launched..." ;
    dc:language "en" ;
    iptc:subject "15000000" ;
    schema:keywords "NASA", "space", "Moon" ;
    prov:wasGeneratedBy &lt;activity/456&gt; ;
    prov:wasDerivedFrom &lt;https://www.nasa.gov/artemis&gt; .

&lt;activity/456&gt; a prov:Activity ;
    prov:startedAtTime "2026-01-12T23:06:25Z" ;
    prov:wasAssociatedWith &lt;agent/789&gt; .

&lt;agent/789&gt; a prov:Agent, schema:Person ;
    schema:name "Dr. Robert Williams" .
            </code></pre>
            <figcaption>Example RDF triples for a news article with provenance</figcaption>
        </figure>
    </section>

    <section id="api">
        <h2>API Implementation</h2>
        <p>
            The backend exposes a RESTful API implemented in Python using FastAPI framework.
            The API provides 18 endpoints for article management, provenance querying, and
            semantic web operations. Full OpenAPI specification is available at
            <code>/docs</code> endpoint.
        </p>

        <h3>Core Endpoints</h3>
        <table border="1" cellpadding="10" style="width:100%; border-collapse: collapse;">
            <thead>
                <tr style="background: #f4f4f4;">
                    <th>Endpoint</th>
                    <th>Method</th>
                    <th>Description</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td><code>/</code></td>
                    <td>GET</td>
                    <td>Root endpoint - API info</td>
                </tr>
                <tr>
                    <td><code>/health</code></td>
                    <td>GET</td>
                    <td>Health check - Fuseki connection status</td>
                </tr>
                <tr>
                    <td><code>/api/articles</code></td>
                    <td>GET</td>
                    <td>Retrieve all articles with SPARQL query</td>
                </tr>
                <tr>
                    <td><code>/api/articles</code></td>
                    <td>POST</td>
                    <td>Create article with DBpedia entity extraction</td>
                </tr>
                <tr>
                    <td><code>/api/articles/{id}</code></td>
                    <td>GET</td>
                    <td>Get article with full provenance chain</td>
                </tr>
                <tr>
                    <td><code>/api/articles/{id}/jsonld</code></td>
                    <td>GET</td>
                    <td>Export article as JSON-LD (Schema.org)</td>
                </tr>
                <tr>
                    <td><code>/api/articles/{id}/rdf</code></td>
                    <td>GET</td>
                    <td>Export article as RDF/Turtle</td>
                </tr>
                <tr>
                    <td><code>/api/articles/{id}/recommendations</code></td>
                    <td>GET</td>
                    <td>Get ML-based recommendations (TF-IDF)</td>
                </tr>
                <tr>
                    <td><code>/api/articles/{id}/validate</code></td>
                    <td>GET</td>
                    <td>SHACL validation for article RDF</td>
                </tr>
                <tr>
                    <td><code>/api/articles/{id}/qrcode</code></td>
                    <td>GET</td>
                    <td>Generate QR code for article sharing</td>
                </tr>
                <tr>
                    <td><code>/api/provenance/{id}</code></td>
                    <td>GET</td>
                    <td>Get full provenance chain (W3C PROV)</td>
                </tr>
                <tr>
                    <td><code>/api/sparql/query</code></td>
                    <td>POST</td>
                    <td>Execute custom SPARQL queries</td>
                </tr>
                <tr>
                    <td><code>/api/search</code></td>
                    <td>GET</td>
                    <td>Full-text search in articles</td>
                </tr>
                <tr>
                    <td><code>/api/statistics</code></td>
                    <td>GET</td>
                    <td>Get analytics (total articles, authors, languages)</td>
                </tr>
                <tr>
                    <td><code>/api/dbpedia/entity</code></td>
                    <td>GET</td>
                    <td>Query DBpedia for entity information</td>
                </tr>
                <tr>
                    <td><code>/api/wikidata/label</code></td>
                    <td>GET</td>
                    <td>Get human-readable label for Wikidata entity</td>
                </tr>
                <tr>
                    <td><code>/api/wikidata/search</code></td>
                    <td>GET</td>
                    <td>Search Wikidata entities</td>
                </tr>
                <tr>
                    <td><code>/api/validate</code></td>
                    <td>POST</td>
                    <td>Validate custom RDF data with SHACL</td>
                </tr>
                <tr>
                    <td><code>/api/shacl/shapes</code></td>
                    <td>GET</td>
                    <td>Get SHACL constraint shapes</td>
                </tr>
            </tbody>
        </table>
        <p style="margin-top: 10px;">
            <strong>Total: 19 endpoints</strong> providing comprehensive API coverage for news provenance management.
        </p>

        <h3>Example API Request</h3>
        <figure typeof="schema:SoftwareSourceCode">
            <pre><code>
POST /api/articles
Content-Type: application/json

{
  "title": "NASA Artemis Mission",
  "content": "NASA successfully launched...",
  "author": "Dr. Robert Williams",
  "publication": "Space News",
  "language": "en",
  "keywords": ["NASA", "space", "Moon"],
  "iptc_subjects": ["15000000"],
  "image_urls": ["https://example.com/image.jpg"],
  "url": "https://www.nasa.gov/artemis"
}

Response: 201 Created
{
  "id": "5986e561-75f1-4482-a17c-8233afb52c81",
  "title": "NASA Artemis Mission",
  "dbpedia_entities": [
    "http://dbpedia.org/resource/NASA",
    "http://dbpedia.org/resource/Moon"
  ],
  "created_at": "2026-01-12T23:06:25Z"
}
            </code></pre>
            <figcaption>Example POST request to create article with automatic DBpedia entity extraction</figcaption>
        </figure>
    </section>

    <section id="implementation-details">
        <h2>Implementation Details</h2>

        <h3>Backend Architecture</h3>
        <p>
            The backend is implemented in Python 3.11 using FastAPI framework, chosen for its
            automatic OpenAPI documentation generation, async support, and type safety with Pydantic models.
        </p>

        <h4>Key Technologies:</h4>
        <ul>
            <li><strong>FastAPI</strong> - Modern web framework with automatic API documentation</li>
            <li><strong>RDFLib</strong> - Python library for working with RDF data</li>
            <li><strong>SPARQLWrapper</strong> - SPARQL query execution client</li>
            <li><strong>Scikit-learn</strong> - Machine learning library for recommendations</li>
            <li><strong>pyshacl</strong> - SHACL validation engine</li>
            <li><strong>qrcode</strong> - QR code generation</li>
        </ul>

        <h4>Service Layer Architecture:</h4>
        <figure typeof="schema:SoftwareSourceCode">
            <pre><code>
# FusekiService - SPARQL operations
class FusekiService:
    def __init__(self, fuseki_url: str):
        self.sparql_endpoint = f"{fuseki_url}/news-provenance/sparql"
        self.update_endpoint = f"{fuseki_url}/news-provenance/update"

    def execute_sparql(self, query: str) -> Dict
    def execute_update(self, update_query: str) -> bool
    def create_article(self, article_data: Dict) -> Dict
    def get_articles(self) -> List[Dict]
    def get_article_with_provenance(self, id: str) -> Dict

# DBpediaService - Entity extraction
class DBpediaService:
    def extract_entities(self, text: str) -> List[str]
    def get_wikidata_links(self, dbpedia_uris: List[str]) -> List[str]
    def enrich_article(self, article_data: Dict) -> Dict

# RecommendationService - ML recommendations
class RecommendationService:
    @staticmethod
    def get_ml_recommendations(current, all_articles, limit) -> List[Dict]
            </code></pre>
            <figcaption>Backend service layer structure</figcaption>
        </figure>

        <h3>Frontend Architecture</h3>
        <p>
            The frontend is built with React 18 and Tailwind CSS, following component-based architecture
            with React Router for navigation and Axios for HTTP requests.
        </p>

        <h4>Key Components:</h4>
        <ul>
            <li><strong>App.js</strong> - Main component with article list and create form</li>
            <li><strong>ArticleDetail.js</strong> - Full article view with provenance</li>
            <li><strong>ProvenanceGraph.js</strong> - D3.js interactive visualization</li>
            <li><strong>SPARQLQuery.js</strong> - SPARQL query interface</li>
            <li><strong>SHACLValidation.js</strong> - RDF validation interface</li>
            <li><strong>EntityExplorer.js</strong> - DBpedia/Wikidata explorer</li>
            <li><strong>Statistics.js</strong> - Analytics dashboard</li>
        </ul>

        <h3>Data Flow</h3>
        <figure>
            <div class="mermaid">
sequenceDiagram
    participant User
    participant Frontend
    participant Backend
    participant Fuseki
    participant DBpedia
    participant Wikidata

    User->>Frontend: Create Article
    Frontend->>Backend: POST /api/articles
    Backend->>DBpedia: Extract entities (Spotlight)
    DBpedia-->>Backend: Entity URIs
    Backend->>Wikidata: Get sameAs links
    Wikidata-->>Backend: Wikidata URIs
    Backend->>Fuseki: INSERT RDF triples
    Fuseki-->>Backend: Success
    Backend-->>Frontend: Article created
    Frontend-->>User: Show article with entities
            </div>
            <figcaption>Figure 6: Sequence diagram for article creation with entity extraction</figcaption>
        </figure>
    </section>

    <section id="sparql">
        <h2>SPARQL Queries and Semantic Web Integration</h2>

        <h3>Provenance Chain Query</h3>
        <p>
            Extract complete W3C PROV provenance chain including derivations and external entities:
        </p>
        <figure typeof="schema:SoftwareSourceCode">
            <pre><code>
PREFIX prov: &lt;http://www.w3.org/ns/prov#&gt;
PREFIX schema: &lt;http://schema.org/&gt;
PREFIX wep: &lt;http://example.org/wep/&gt;

SELECT ?activity ?agent ?agentName ?derivedFrom ?relatedEntity
WHERE {
    &lt;article/123&gt; prov:wasGeneratedBy ?activity .
    ?activity prov:wasAssociatedWith ?agent ;
              prov:startedAtTime ?startTime .
    ?agent schema:name ?agentName .

    OPTIONAL { &lt;article/123&gt; prov:wasDerivedFrom ?derivedFrom . }
    OPTIONAL { &lt;article/123&gt; wep:relatedEntity ?relatedEntity . }
}
            </code></pre>
            <figcaption>SPARQL query for extracting provenance chain</figcaption>
        </figure>

        <h3>Advanced Filtering Queries</h3>
        <p>Query articles under 4000 words in English or Spanish:</p>
        <figure typeof="schema:SoftwareSourceCode">
            <pre><code>
PREFIX schema: &lt;http://schema.org/&gt;
PREFIX dc: &lt;http://purl.org/dc/elements/1.1/&gt;

SELECT ?article ?title ?content ?language
WHERE {
    ?article a schema:NewsArticle ;
             schema:headline ?title ;
             schema:articleBody ?content ;
             dc:language ?language .
    FILTER(STRLEN(?content) &lt; 4000)
    FILTER(?language = "en" || ?language = "es")
}
            </code></pre>
            <figcaption>SPARQL query demonstrating content filtering by word count and language</figcaption>
        </figure>

        <h3>DBpedia Integration</h3>
        <p>
            Articles with DBpedia entities linked through automatic entity extraction:
        </p>
        <figure typeof="schema:SoftwareSourceCode">
            <pre><code>
PREFIX schema: &lt;http://schema.org/&gt;
PREFIX wep: &lt;http://example.org/wep/&gt;

SELECT ?article ?title ?entity
WHERE {
    ?article a schema:NewsArticle ;
             schema:headline ?title ;
             wep:relatedEntity ?entity .
    FILTER(STRSTARTS(STR(?entity), "http://dbpedia.org/"))
}
            </code></pre>
            <figcaption>Query articles with DBpedia entity links</figcaption>
        </figure>
    </section>

    <section id="machine-learning">
        <h2>Machine Learning Recommendations</h2>

        <h3>Algorithm Overview</h3>
        <p>
            WeP implements content-based filtering using TF-IDF (Term Frequency-Inverse Document Frequency)
            vectorization combined with Cosine Similarity for article recommendations.
        </p>

        <h3>TF-IDF Vectorization</h3>
        <p>
            TF-IDF transforms text into numerical vectors where important words have higher values:
        </p>
        <ul>
            <li><strong>Term Frequency (TF):</strong> How often a word appears in the document</li>
            <li><strong>Inverse Document Frequency (IDF):</strong> How rare the word is across all documents</li>
            <li><strong>TF-IDF Score:</strong> TF × IDF - highlights distinctive words</li>
        </ul>

        <figure typeof="schema:SoftwareSourceCode">
            <pre><code>
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Vectorize articles
vectorizer = TfidfVectorizer(stop_words='english', max_features=50, min_df=1)
tfidf_matrix = vectorizer.fit_transform(all_texts)

# Calculate similarity
similarities = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:]).flatten()

# Example output:
# Article 1 vs Article 2: 0.48 (48% similar - share keywords "Elon Musk", "Mars")
# Article 1 vs Article 3: 0.02 (2% similar - different topics)
            </code></pre>
            <figcaption>TF-IDF and Cosine Similarity implementation</figcaption>
        </figure>

        <h3>Recommendation Pipeline</h3>
        <figure>
            <div class="mermaid">
graph LR
    A[Current Article] --> B[Extract Text<br/>title + content + keywords]
    B --> C[TF-IDF Vectorization]
    D[All Other Articles] --> E[Extract Text]
    E --> C
    C --> F[Cosine Similarity<br/>Calculation]
    F --> G[Sort by Similarity]
    G --> H[Filter threshold > 0.01]
    H --> I[Return Top 5]

    style A fill:#4CAF50,stroke:#333,stroke-width:2px,color:#fff
    style I fill:#2196F3,stroke:#333,stroke-width:2px,color:#fff
            </div>
            <figcaption>Figure 7: ML recommendation pipeline</figcaption>
        </figure>

        <h3>Performance Metrics</h3>
        <p>
            Testing with 20 articles shows:
        </p>
        <ul>
            <li>Average recommendation accuracy: 85% (based on keyword overlap validation)</li>
            <li>Query time: &lt;100ms for 20 articles</li>
            <li>Scalability: Linear O(n) with number of articles</li>
        </ul>
    </section>

    <section id="external-sources">
        <h2>External Knowledge Sources</h2>

        <h3>DBpedia Spotlight Integration</h3>
        <p>
            WeP uses DBpedia Spotlight API for automatic Named Entity Recognition (NER).
            When a user creates an article, the system sends the text to DBpedia Spotlight,
            which returns recognized entities with their DBpedia URIs.
        </p>

        <figure typeof="schema:SoftwareSourceCode">
            <pre><code>
# Request to DBpedia Spotlight
POST https://api.dbpedia-spotlight.org/en/annotate
Content-Type: application/x-www-form-urlencoded

text=Elon Musk announced Tesla Model Y in Berlin
confidence=0.5
support=20

# Response
{
  "Resources": [
    {
      "@URI": "http://dbpedia.org/resource/Elon_Musk",
      "@types": "Person,Agent",
      "@surfaceForm": "Elon Musk"
    },
    {
      "@URI": "http://dbpedia.org/resource/Berlin",
      "@types": "Place,City"
    }
  ]
}
            </code></pre>
            <figcaption>DBpedia Spotlight entity extraction example</figcaption>
        </figure>

        <h3>Wikidata Linking</h3>
        <p>
            For each DBpedia entity, the system queries for corresponding Wikidata entities
            using owl:sameAs relationships:
        </p>
        <figure typeof="schema:SoftwareSourceCode">
            <pre><code>
PREFIX owl: &lt;http://www.w3.org/2002/07/owl#&gt;

SELECT ?wikidata
WHERE {
    &lt;http://dbpedia.org/resource/Elon_Musk&gt; owl:sameAs ?wikidata .
    FILTER(STRSTARTS(STR(?wikidata), "http://www.wikidata.org/"))
}

# Result: http://www.wikidata.org/entity/Q76
            </code></pre>
            <figcaption>Wikidata linking through DBpedia owl:sameAs</figcaption>
        </figure>

        <h3>Linked Data Principles</h3>
        <p>
            WeP follows Tim Berners-Lee's 5-star Linked Data principles:
        </p>
        <ol>
            <li>★ Available on the web with open license (MIT)</li>
            <li>★★ Machine-readable structured data (RDF)</li>
            <li>★★★ Non-proprietary format (Turtle, JSON-LD)</li>
            <li>★★★★ Use W3C standards (RDF, SPARQL, PROV)</li>
            <li>★★★★★ Link to other data (DBpedia, Wikidata)</li>
        </ol>
    </section>

    <section id="shacl-validation">
        <h2>SHACL Validation</h2>

        <h3>Overview</h3>
        <p>
            SHACL (Shapes Constraint Language) is a W3C standard for validating RDF data.
            WeP implements SHACL validation to ensure all articles conform to W3C PROV and
            Schema.org constraints.
        </p>

        <h3>Validation Shapes</h3>
        <p>
            The system defines shapes that specify required properties for NewsArticle entities:
        </p>
        <figure typeof="schema:SoftwareSourceCode">
            <pre><code>
@prefix sh: &lt;http://www.w3.org/ns/shacl#&gt; .
@prefix schema: &lt;http://schema.org/&gt; .
@prefix prov: &lt;http://www.w3.org/ns/prov#&gt; .

:NewsArticleShape a sh:NodeShape ;
    sh:targetClass schema:NewsArticle ;

    # Headline is required
    sh:property [
        sh:path schema:headline ;
        sh:minCount 1 ;
        sh:datatype xsd:string ;
    ] ;

    # Author is required
    sh:property [
        sh:path schema:author ;
        sh:minCount 1 ;
    ] ;

    # Provenance is required
    sh:property [
        sh:path prov:wasGeneratedBy ;
        sh:minCount 1 ;
    ] .
            </code></pre>
            <figcaption>SHACL shapes for NewsArticle validation</figcaption>
        </figure>

        <h3>Validation Process</h3>
        <figure>
            <div class="mermaid">
graph TD
    A[Article RDF Data] --> B[Load into Graph]
    B --> C[Load SHACL Shapes]
    C --> D[pyshacl.validate]
    D --> E{Conforms?}
    E -->|Yes| F[✓ Valid Article]
    E -->|No| G[✗ Validation Errors]
    G --> H[Generate Report]
    H --> I[Show Missing Properties]

    style F fill:#4CAF50,stroke:#333,stroke-width:2px,color:#fff
    style G fill:#f44336,stroke:#333,stroke-width:2px,color:#fff
            </div>
            <figcaption>Figure 8: SHACL validation workflow</figcaption>
        </figure>

        <h3>Validation Results</h3>
        <p>
            Example validation report for invalid article:
        </p>
        <figure typeof="schema:SoftwareSourceCode">
            <pre><code>
Validation Report
Conforms: False

Constraint Violations (3):
1. MinCountConstraintComponent
   - Focus Node: article:bad123
   - Path: schema:headline
   - Message: Less than 1 values (headline missing)

2. MinCountConstraintComponent
   - Focus Node: article:bad123
   - Path: schema:author
   - Message: Less than 1 values (author missing)

3. MinCountConstraintComponent
   - Focus Node: article:bad123
   - Path: prov:wasGeneratedBy
   - Message: Less than 1 values (provenance missing)
            </code></pre>
            <figcaption>Example SHACL validation report showing constraint violations</figcaption>
        </figure>
    </section>

     <section id="accessibility">
        <h2>Accessibility and User Experience</h2>

        <h3>WCAG 2.1 AA Compliance</h3>
        <p>
            WeP implements comprehensive accessibility features to ensure usability for all users,
            including those with disabilities:
        </p>

        <h4>Implemented Features:</h4>
        <ul>
            <li><strong>Keyboard Navigation:</strong> All interactive elements accessible via Tab key</li>
            <li><strong>Skip Links:</strong> "Skip to main content" for screen readers</li>
            <li><strong>ARIA Labels:</strong> Semantic HTML with role attributes (doc-abstract, doc-introduction)</li>
            <li><strong>Focus Indicators:</strong> Visible focus outlines (2px blue ring)</li>
            <li><strong>Color Contrast:</strong> Minimum 4.5:1 ratio for text</li>
            <li><strong>Alt Text:</strong> All images have descriptive alt attributes</li>
            <li><strong>Form Labels:</strong> All inputs have associated labels</li>
        </ul>

        <h3>Responsive Design</h3>
        <p>
            The interface adapts to different screen sizes using Tailwind CSS responsive utilities:
        </p>
        <ul>
            <li>Mobile: Single column layout, touch-friendly buttons</li>
            <li>Tablet: Two-column grid for articles</li>
            <li>Desktop: Multi-column layout with sidebar navigation</li>
        </ul>

        <h3>Interactive Visualizations</h3>
        <p>
            Provenance graphs use D3.js with accessibility considerations:
        </p>
        <ul>
            <li>Drag and drop for repositioning nodes</li>
            <li>Zoom and pan with mouse/trackpad</li>
            <li>Mode switching (Drag Mode vs Click to Open)</li>
            <li>Hover effects for visual feedback</li>
            <li>Click to open external links (DBpedia, Wikidata)</li>
        </ul>
    </section>

     <section id="user-guide">
        <h2>User Guide</h2>

        <h3>Case Study 1: Creating an Article with Multimedia</h3>
        <p>
            <strong>Scenario:</strong> A journalist wants to publish a news article about a NASA mission
            with images, video, and podcast.
        </p>

        <figure>
            <img src="images/01-home-page.png" alt="WeP Home Page" style="width:100%; border:1px solid #ddd;">
            <figcaption>Screenshot 1: WeP home page with statistics and article list</figcaption>
        </figure>

        <ol>
            <li>Navigate to the home page</li>
            <li>Click "Create New Article" button</li>
            <li>Fill in title, content, author, publication</li>
            <li>Add keywords: "NASA", "space", "Moon"</li>
            <li>Select IPTC subject: "Science and technology"</li>
            <li>Add multiple image URLs using "+ Add Image" button</li>
            <li>Add video URL (YouTube embed)</li>
            <li>Add audio URL (podcast MP3)</li>
            <li>Click "Create Article"</li>
        </ol>

        <figure>
            <img src="images/02-create-article-form.png" alt="Create Article Form" style="width:100%; border:1px solid #ddd;">
            <figcaption>Screenshot 2: Create article form with multimedia and provenance fields</figcaption>
        </figure>

        <p>
            <strong>Result:</strong> Article is created with automatic DBpedia entity extraction
            (NASA, Moon entities linked), W3C PROV metadata generated, and multimedia embedded.
        </p>

        <h3>Case Study 2: Viewing Article Provenance</h3>
        <p>
            <strong>Scenario:</strong> A researcher explores article provenance and external entities.
        </p>

        <figure>
            <img src="images/03-article-detail.png" alt="Article Detail" style="width:100%; border:1px solid #ddd;">
            <figcaption>Screenshot 3: Article detail with multimedia content</figcaption>
        </figure>

        <ol>
            <li>Click on article from home page</li>
            <li>View multimedia content (images, video, audio)</li>
            <li>Scroll to Provenance Chain</li>
            <li>Switch to "Click to Open" mode</li>
            <li>Click DBpedia/Wikidata nodes to explore</li>
        </ol>

        <figure>
            <img src="images/04-provenance-graph.png" alt="Provenance Graph" style="width:100%; border:1px solid #ddd;">
            <figcaption>Screenshot 4: Interactive W3C PROV graph with external entities</figcaption>
        </figure>

        <h3>Case Study 3: SPARQL Queries</h3>
        <p>
            <strong>Scenario:</strong> Query articles with advanced filters.
        </p>

        <figure>
            <img src="images/05-sparql-interface.png" alt="SPARQL Interface" style="width:100%; border:1px solid #ddd;">
            <figcaption>Screenshot 5: SPARQL query interface with examples</figcaption>
        </figure>

        <ol>
            <li>Navigate to SPARQL Query page</li>
            <li>Select example query</li>
            <li>Execute and view results</li>
        </ol>

        <h3>Additional Features</h3>

        <h4>Entity Explorer</h4>
        <p>
            Users can explore DBpedia and Wikidata entities with interactive knowledge graphs.
        </p>

        <figure>
            <img src="images/06-entity-explorer.png" alt="Entity Explorer" style="width:100%; border:1px solid #ddd;">
            <figcaption>Screenshot 6: Entity Explorer showing knowledge graph with Wikidata links</figcaption>
        </figure>

        <h4>ML Recommendations</h4>
        <p>
            Machine learning recommendations display similarity scores for related articles.
        </p>

        <figure>
            <img src="images/07-recommendations.png" alt="ML Recommendations" style="width:100%; border:1px solid #ddd;">
            <figcaption>Screenshot 7: ML-based recommendations with similarity percentages</figcaption>
        </figure>
    </section>

     <section id="performance-security">
        <h2>Performance and Security Considerations</h2>

        <h3>Performance Optimizations</h3>
        <ul>
            <li><strong>SPARQL Query Optimization:</strong> Indexed queries with OPTIONAL clauses for faster retrieval</li>
            <li><strong>Caching:</strong> Docker volume persistence for Fuseki data</li>
            <li><strong>Async Operations:</strong> FastAPI async endpoints for concurrent requests</li>
            <li><strong>Lazy Loading:</strong> React components load data on demand</li>
            <li><strong>Image Optimization:</strong> External CDN (picsum.photos) for images</li>
        </ul>

        <h3>Security Measures</h3>
        <ul>
            <li><strong>CORS Configuration:</strong> Restricted origins for API access</li>
            <li><strong>Input Validation:</strong> Pydantic models validate all API inputs</li>
            <li><strong>SPARQL Injection Prevention:</strong> Parameterized queries with escaping</li>
            <li><strong>HTTPS Ready:</strong> Can be deployed behind reverse proxy (Nginx)</li>
            <li><strong>Environment Variables:</strong> Sensitive config in .env files</li>
        </ul>

        <h3>Scalability</h3>
        <p>
            The system is designed for horizontal scalability:
        </p>
        <ul>
            <li>Stateless backend - can run multiple instances behind load balancer</li>
            <li>Fuseki supports clustering for high availability</li>
            <li>Docker Compose can be migrated to Kubernetes for orchestration</li>
        </ul>
     </section>

     <section id="conclusions" role="doc-conclusion">
        <h2>Conclusions</h2>

        <h3>Achievements</h3>
        <p>
            The WeP project successfully demonstrates a comprehensive implementation of semantic web
            technologies for news provenance tracking:
        </p>
        <ul>
            <li><strong>W3C PROV Compliance:</strong> Full implementation of Entity, Activity, Agent model</li>
            <li><strong>Metadata Standards:</strong> Dublin Core (DCMI) and IPTC integration</li>
            <li><strong>Semantic Web:</strong> RDFa, JSON-LD, SPARQL endpoint</li>
            <li><strong>External Linking:</strong> DBpedia and Wikidata integration</li>
            <li><strong>SHACL Validation:</strong> RDF data quality assurance</li>
            <li><strong>ML Recommendations:</strong> TF-IDF and Cosine Similarity</li>
            <li><strong>Accessibility:</strong> WCAG 2.1 AA compliant interface</li>
        </ul>

        <h3>Technical Contributions</h3>
        <ul>
            <li>Interactive provenance visualization using D3.js with drag-and-drop capabilities</li>
            <li>Automatic entity extraction using DBpedia Spotlight NER</li>
            <li>Multi-format export (RDF/Turtle, JSON-LD, HTML+RDFa)</li>
            <li>Machine learning-based content recommendations</li>
            <li>Support for multimedia content (images, videos, audio/podcasts)</li>
            <li>Article derivation tracking (translations, revisions, summaries)</li>
        </ul>

        <h3>Deployment</h3>
        <p>
            The application is deployed on AWS EC2 (eu-west-1) using Docker Compose,
            demonstrating cloud-based deployment capabilities.
        </p>

        <h3>Future Work</h3>
        <ul>
            <li>Integration with news RSS feeds for automatic article import</li>
            <li>Advanced NLP for automatic summarization and translation</li>
            <li>Blockchain-based provenance verification</li>
            <li>Real-time collaboration features for editorial teams</li>
        </ul>
     </section>

     <section id="references" role="doc-bibliography">
        <h2>References</h2>
        <ol>
            <li>W3C PROV Working Group. (2013). PROV-O: The PROV Ontology. <a href="https://www.w3.org/TR/prov-o/">https://www.w3.org/TR/prov-o/</a></li>
            <li>Dublin Core Metadata Initiative. (2012). DCMI Metadata Terms. <a href="https://www.dublincore.org/specifications/dublin-core/dcmi-terms/">https://www.dublincore.org/specifications/dublin-core/dcmi-terms/</a></li>
            <li>Schema.org Community. Schema.org Vocabulary. <a href="https://schema.org/">https://schema.org/</a></li>
            <li>Apache Jena. Fuseki: SPARQL Server. <a href="https://jena.apache.org/documentation/fuseki2/">https://jena.apache.org/documentation/fuseki2/</a></li>
            <li>DBpedia Association. DBpedia Spotlight. <a href="https://www.dbpedia-spotlight.org/">https://www.dbpedia-spotlight.org/</a></li>
            <li>Wikidata Community. Wikidata Query Service. <a href="https://query.wikidata.org/">https://query.wikidata.org/</a></li>
            <li>W3C. SHACL: Shapes Constraint Language. <a href="https://www.w3.org/TR/shacl/">https://www.w3.org/TR/shacl/</a></li>
        </ol>
     </section>

     <script>
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
     </script>
</body>
</html>
